{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Web scraping application with Python using Scrapy**\n",
    "\n",
    "### **Extracting current world population data from a website**\n",
    "\n",
    "<img src='./img/portada_scrapy.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Consideraciones legales y éticas**\n",
    "\n",
    "Esta publicación no trata sobre cómo extraer datos de una página web con fines ilegales.\n",
    "\n",
    "Hay que asegurarse de tener permiso antes de extraer ciertos tipos de datos que puede violar los términos del servicio o incluso regulaciones legales:\n",
    "\n",
    "- Revise los términos de uso de la página web en relación a los permisos de extracción de datos.\n",
    "- Priorice el uso de las APIs, si están disponibles, ya que proporcionan acceso legal a los datos.\n",
    "- Póngase en contacto directamente con el propietario de la página web para comprobar el permiso de extracción de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Introducción**\n",
    "\n",
    "En este proyecto, veremos cómo se pueden obtener las poblaciones actuales de los diferentes países o territorios dependientes del mundo, aplicando Scrapy. Además de la población, también obtendremos datos como:\n",
    "- Porcentaje de cambio anual.\n",
    "- Cambio Neto.\n",
    "- Densidad de población.\n",
    "- Superficie terrestre.\n",
    "- Número de migrantes (neto).\n",
    "- Tasa de fertilidad.\n",
    "- Edad media.\n",
    "- Porcentaje de población urbana.\n",
    "- Porcentaje de población que representan en el mundo.\n",
    "\n",
    "Estos datos han sido elaborados por las Naciones Unidas (Departamento de Asuntos Económicos y sociales, División de Población) y publicados por [Worldometers](https://www.worldometers.info/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Conocimientos previos sobre Scrapy**\n",
    "\n",
    "Scrapy es uno de los frameworks más potentes para realizar web scraping utilizando Python. Está diseñada para la extracción de datos a gran escala, de forma rápida y eficiente. \n",
    "\n",
    "El proceso implica realizar solicitudes HTTP a páginas web, analizar el contenido HTML devuelto y extraer la información requerida.\n",
    "\n",
    "\n",
    "**Características del Scrapy**\n",
    "\n",
    "- *Rendimiento y eficiencia*: Está diseñado para ser rápido y eficiente, ya que maneja múltiples solicitudes en paralelo de manera asíncrona utilizando Twisted, un motor de red asincrónica, lo que lo hace extremadamente rápido.\n",
    "\n",
    "- *Contenido dinámico*: los sitios web con contenido cargado dinámicamente mediante JavaScript pueden ser difíciles de extraer, ya que los datos que necesita podrían no estar presentes en la respuesta HTML inicial.\n",
    "\n",
    "- *Arquitectura extensible*: Scrapy tiene una arquitectura basada en middleware que permite personalizar el proceso de scraping fácilmente.\n",
    "\n",
    "- *Control de flujo de datos*: Permite manejar las solicitudes y las respuestas de una manera estructurada, filtrando datos y manejando errores sin detener el proceso.\n",
    "\n",
    "- *Soporte para politeness*: Scrapy puede respetar las reglas de robots.txt, lo que permite ajustar la frecuencia de las solicitudes para no sobrecargar los servidores web. Muchos sitios web tienen defensas contra el scraping, como el bloqueo de las direcciones IP de los clientes que realizan demasiadas solicitudes en un período corto.\n",
    "\n",
    "- *Fácil integración con herramientas de almacenamiento*: Puedes guardar los datos extraídos en varios formatos como JSON, XML, CSV, o directamente en bases de datos SQL o NoSQL.\n",
    "\n",
    "- *Integración con otras herramientas*: Se puede integrar con bases de datos, servicios en la nube y herramientas como Splash o Selenium para manejar contenido dinámico generado con JavaScript.\n",
    "\n",
    "- *Gestión de Cookies y Sesiones*: Maneja automáticamente las cookies y mantiene sesiones durante el scraping.\n",
    "\n",
    "**Arquitectura modular del Scrapy**\n",
    "\n",
    "- *Spiders*: Son clases donde definimos cómo Scrapy debe navegar por un sitio web y extraer datos. Aquí especificamos la URL que se debe rastrear y cómo extraer la información. \n",
    "\n",
    "- *Crawlers* (rastreadores): son responsables de iniciar el proceso de scraping y gestionar las solicitudes hacia los diferentes sitios web. Scrapy incluye un administrador de rastreadores muy eficiente.\n",
    "\n",
    "- *Selectors*: Utilizan XPath o CSS para localizar y extraer datos específicos de las páginas web.\n",
    "\n",
    "- *Item*: es un contenedor donde se almacenan los datos que has extraído. Es una representación de los datos de un sitio web.\n",
    "\n",
    "- *Pipeline*: Procesa los datos extraídos que han sido definidos en los items, permitiendo limpiar, validar y almacenar la información.\n",
    "\n",
    "- *Middlewares*: Permiten personalizar el comportamiento de Scrapy. Son componentes que nos permiten modificar o personalizar las solicitudes y las respuestas en el proceso de scraping. Scrapy tiene middlewares para manejar cookies, cabeceras, user agents, y proxies, entre otras cosas.\n",
    "\n",
    "**Flujo de trabajo en Scrapy**\n",
    "\n",
    "- *Inicio del spider*: El spider comienza con una lista de URLs que se definirán en `start_urls` o a través de una función `start_requests`.\n",
    "\n",
    "- *Solicitudes de URLs*: El spider envía solicitudes HTTP a las URLs especificadas. Scrapy descarga las páginas y las pasa a la función `parse`.\n",
    "\n",
    "- *Extracción de datos*: En la función `parse`, el spider analiza el contenido de la página y extrae los datos que se necesitan utilizando `selectores de XPath o CSS`. Estos datos se almacenan en `items`.\n",
    "\n",
    "- *Procesamiento de datos*: Los datos extraídos pasan por `pipelines` de procesamiento, donde se pueden limpiar, validar y almacenar en el formato deseado.\n",
    "\n",
    "- *Navegación de enlaces*: Si la página contiene más enlaces que el spider debe visitar, estas URLs se envían nuevamente a Scrapy para continuar el proceso de scraping de manera recursiva.\n",
    "\n",
    "**Scrapy Shell**\n",
    "\n",
    "El Scrapy Shell es una característica muy útil para depurar y probar selectores en el proceso de scraping. Es una consola interactiva en la que puedes cargar una página y experimentar con las consultas antes de escribir el spider completo. Por ejemplo, puedes usar selectores CSS o XPath directamente en la consola para verificar que extraen la información correcta:\n",
    "\n",
    "`scrapy shell 'http://ejemplo.com/page/1/'`\n",
    "\n",
    "**Casos de uso**\n",
    "\n",
    "- Extracción de precios de productos en sitios de e-commerce.\n",
    "- Monitorización de sitios web para la detección de cambios en el contenido.\n",
    "- Extracción de datos de investigaciones en grandes bases de datos públicas.\n",
    "- Análisis de datos sociales para recopilar información desde foros, redes sociales, etc.\n",
    "\n",
    "**Desafíos y consideraciones éticas**\n",
    "\n",
    "- *Bloqueo de scraping*: Muchos sitios web utilizan mecanismos para bloquear bots, como CAPTCHAs, bloqueos por IP, o scripts de JavaScript. Para resolver esto, Scrapy permite integrar proxies rotativos y manejar cookies de forma avanzada.\n",
    "- *Legales y éticas*: Es importante respetar los términos de uso de los sitios web y las reglas de robots.txt, ya que algunos sitios prohíben o limitan el scraping. Asegúrate de que tienes permiso para extraer y utilizar los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Instalación de dependencias**\n",
    "\n",
    "A continuación se muestran las librerías necesarias para la ejecución del código creado:\n",
    "\n",
    "- ipykernel : kernel de Jupyter necesario para ejecutar Python.\n",
    "- tqdm : herramienta útil para crear barras de progreso en bucles.\n",
    "- joblib : biblioteca que permite la serialización de objetos Python.\n",
    "- scipy : biblioteca para el cálculo avanzado que permite manipular y visualizar datos de alto nivel.\n",
    "- pandas : biblioteca escrita sobre Numpy para la manipulación y el análisis de datos.\n",
    "\n",
    "Ejecutamos en la terminal el siguiente código:\n",
    "\n",
    "`pip install ipykernel tqdm joblib scipy pandas`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Instalación de Scrapy**\n",
    "\n",
    "Estoy trabajando en un entorno conda en el que ya está instalado python, en concreto la versión 3.12.2.\n",
    "\n",
    "Para instalar scrapy ejecutamos:\n",
    "\n",
    "`pip install scrapy`\n",
    "\n",
    "Podemos comprobar que la instalación fue exitosa mirando la versión instalada. En mi caso es la versión 2.11.2:\n",
    "\n",
    "`scrapy version`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Archivo robots.txt**\n",
    "\n",
    "En este archivo están las categorías que están dehabilitadas en la web.\n",
    "\n",
    "En el navegador escribimos: https://www.worldometers.info/robots.txt y obtenemos:\n",
    "\n",
    "<img src = './img/robots.jpg' windth = 800>\n",
    "\n",
    "Eso quiere decir que este archivo no existe en esta página y por lo tanto no tiene nada deshabilitado. Podemos acceder a cualquier dato de la página web.\n",
    "\n",
    "En caso de que sí hubiesen categorías deshabilitadas, scrapy usaría el archivo robots.txt para evitar tener problemas legales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Comenzamos el proyecto**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la terminal nos ubicamos en la ruta en la que queremos crear la carpeta que contendrá nuestro proyecto y ejecutamos:\n",
    "\n",
    "`scrapy startproject población_mundial`\n",
    "\n",
    "Se creará una carpeta cuya estructura interior tendrá el siguiente aspecto:\n",
    "\n",
    "<img src = './img/estructura_scrapy.jpg' windth = 800>\n",
    "\n",
    "Vemos qué es cada uno de los archivos que se han creado:\n",
    "\n",
    "- `scrapy.cfg`: el archivo de configuración del proyecto.\n",
    "- `items.py`: archivo de elementos del proyecto, define los modelos para los datos extraídos.\n",
    "- `middlewares.py`: archivo de middleware del proyecto, que se conecta al procesamiento de solicitud/respuesta de Scrapy.\n",
    "- `pipelines.py`: archivo de pipelines del proyecto, procesa los elementos después de que hayan sido raspados por el spider.\n",
    "- `settings.py`: Archivo de configuración del proyecto, configura los ajustes para su proyecto Scrapy.\n",
    "- `spiders/`: Directorio donde almacenarás tus spiders.\n",
    "\n",
    "Algo muy importante es que comprobemos que en el archivo \"settings.py\" la opción \"ROBOTSTXT_OBEY\" esté activada, para que scrapy tenga en cuenta el archivo \"robots.txt\" antes de comenzar el web scraping.\n",
    "\n",
    "<img src = './img/settings.jpg' width = 800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Inspección de la web**\n",
    "\n",
    "Con el lenguaje de XPath (consola JavaScript) vamos a ver cómo extraemos la información.\n",
    "\n",
    "Abrimos la página de la que queremos extraer los datos: 'https://www.worldometers.info/world-population/population-by-country/'\n",
    "\n",
    "Inspeccionando la web, podemos observar que nuestra tabla de datos se encuentra dentro de la etiqueta `<table>` y que tiene dos partes un \"head\" y un \"body\".\n",
    "\n",
    "<img src = './img/table.jpg' windth = 800>\n",
    "\n",
    "Dentro del \"head\" nos encontramos con un `<tr>` ya que el head sólo tiene una fila y en el interior del mismo nos encontramos con 12 `<th>` que es cada una de las columnas que tiene el encabezado de la tabla.\n",
    "\n",
    "Dentro del body nos encontramos con 234 `<tr>` que es el número de filas que tiene el body y dentro de cada uno de ellos hay 12 `<td>` que son el número de columnas.\n",
    "\n",
    "Para localizar los elementos de una página web, Scrapy dispone de dos métodos: XPath y los selectores CSS. En este artículo haremos uso de los XPath.\n",
    "\n",
    "Para localizar los XPath de los elementos: botón derecho sobre el elemento, copiar/copiar XPath. Por ejemplo si queremos localizar el pais de la India, obtenemos:\n",
    "\n",
    "`//*[@id=\"example2\"]/tbody/tr[1]/td[2]/a`\n",
    "\n",
    "<img src = './img/copiar_xpath.jpg' windth = 800>\n",
    "\n",
    "Vamos a la consola javascrip y definimos el XPath para ver que nos devuelve el resultado correcto:\n",
    "\n",
    "<img src = './img/consola_xpath.jpg' windth = 800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Definimos los XPath**\n",
    "\n",
    "**Nombre de las columnas**\n",
    "\n",
    "`$x('//table//thead//tr//th//text()').map(elm => elm.wholeText)`\n",
    "\n",
    "Resultados que me devuelve la consola:\n",
    "\n",
    "['#', 'Country (or dependency)', 'Population', ' (2024)', 'Yearly', ' Change', 'Net', ' Change', 'Density', ' (P/Km²)', 'Land Area', ' (Km²)', 'Migrants', ' (net)', 'Fert.', ' Rate', 'Med.', ' Age', 'Urban', ' Pop %', 'World', ' Share']\n",
    "\n",
    "Vemos que posteriormente vamos a tener que manipular esta lista ya que me devuelve los nombres de las columnas, pero tengo que juntar algunos.\n",
    "\n",
    "**Valores de las filas**\n",
    "\n",
    "Primer fila:\n",
    "\n",
    "`$x('//table//tbody//tr[1]//td//text()').map(elm => elm.wholeText)`\n",
    "\n",
    "Resultados que me devuelve la consola:\n",
    "\n",
    "['1', 'India', '1,450,935,791', '0.89 %', '12,866,195', '488', '2,973,190', '-630,830', '2.0', '28', '37 %', '17.78 %']\n",
    "\n",
    "Cuarta fila:\n",
    "\n",
    "`$x('//table//tbody//tr[4]//td//text()').map(elm => elm.wholeText)`\n",
    "\n",
    "Resultados que me devuelve la consola:\n",
    "\n",
    "['4', 'Indonesia', '283,487,931', '0.82 %', '2,297,864', '156', '1,811,570', '-38,469', '2.1', '30', '59 %', '3.47 %']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Creación del spider**\n",
    "\n",
    "Crearemos el spider para extraer información de la web.\n",
    "\n",
    "En la terminal vamos al directorio \"spiders\" ejecutando el código:\n",
    "\n",
    "`cd población_mundial/población_mundial/spiders`\n",
    "\n",
    "Creamos el spider:\n",
    "\n",
    "`echo. > spider_poblacion.py`\n",
    "\n",
    "Dentro definimos la clase \"poblacionmundial\" y las funciones necesarias para poder utilizarla. Vemos el contenido final del spider:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import os\n",
    "from csv import DictWriter \n",
    "\n",
    "def append_header_as_row(file_name, field_names):\n",
    "    \"\"\" \n",
    "    Abre un csv y agrega los nombres de las columnas (headers)\n",
    "    file_name es el nombre del csv que abrimos y\n",
    "    field_names son los headers que añadimos como una lista\n",
    "    \"\"\"\n",
    "    # Compruebo si el archivo existe\n",
    "    file_existe = os.path.isfile(file_name)\n",
    "    \n",
    "    # Si existe, se elimina\n",
    "    if file_existe:\n",
    "        os.remove(file_name)\n",
    "        print(f'Archivo {file_name} eliminado.')       \n",
    "\n",
    "    # Abre el archivo en modo 'append'\n",
    "    with open(file_name, 'a+', newline='', encoding='utf-8') as write_obj:\n",
    "        # Crea un objeto 'writer' desde el módulo csv\n",
    "        dict_writer = DictWriter(write_obj, fieldnames=field_names)\n",
    "        # Añade la cabecera al objeto\n",
    "        dict_writer.writeheader()  \n",
    "        \n",
    "def append_dict_as_row(file_name, dict_of_elem, field_names):\n",
    "    \"\"\" \n",
    "    Abre un csv y agrega una fila en forma de diccionario en el csv\n",
    "    file_name es el nombre del csv que abrimos,\n",
    "    dict_of_elem es un dictado con los elementos de la fila que se va a añadir al final del csv\n",
    "    y field_names son los nombres de las columnas (headers)\n",
    "    \"\"\"\n",
    "    # Abre el archivo en modo 'append'\n",
    "    with open(file_name, 'a+', newline='', encoding='utf-8') as write_obj:\n",
    "        # Crea un objeto 'writer' desde el módulo csv\n",
    "        dict_writer = DictWriter(write_obj, fieldnames=field_names)\n",
    "        #  Añade un diccionario de filas al csv\n",
    "        dict_writer.writerow(dict_of_elem)\n",
    "\n",
    "\n",
    "class poblacionmundial(scrapy.Spider):\n",
    "    \n",
    "    name = 'pob_mundial' # nombre del spider\n",
    "    #custom_settings = {'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7'} # activarlo si da problemas\n",
    "    start_urls= [\"https://www.worldometers.info/world-population/population-by-country/\"] # Lista de URL desde las que comienza el scraper\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\" # usamos un user-agent para que amazon no nos bloquee (sino obtendríamos un error 503). Se pueden añadir otros.\n",
    "    }\n",
    "    custom_settings = {\n",
    "        #'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\n",
    "        #'FEED_URI': 'población_mundial_2024.csv', # nombre del archivo donde se guardan los datos\n",
    "        #'FEED_FORMAT': 'csv', # formato del archivo\n",
    "        'ROBOTSTXT_OBEY': True, # validamos el archivo robots.txt\n",
    "        #'FEED_EXPORT_ENCODING': 'utf-8' # codificación usada en la exportación\n",
    "    }\n",
    "\n",
    "    \n",
    "    def parse(self, response):\n",
    "        \"\"\" \n",
    "        Es la función que hará el análisis y obtendrá los datos\n",
    "        \"\"\"\n",
    "        \n",
    "        # Extraer cabeceras de la tabla\n",
    "        columnas = response.xpath('//table//thead//tr//th//text()').getall()\n",
    "        columnas = [columnas[0]] + [columnas[1]] + [columnas[2]+columnas[3]] + [columnas[4]+columnas[5]] + [columnas[6]+columnas[7]] + [columnas[8]+columnas[9]] + [columnas[10]+columnas[11]] + [columnas[12]+columnas[13]] + [columnas[14]+columnas[15]] + [columnas[16]+columnas[17]] + [columnas[18]+columnas[19]] + [columnas[20]+columnas[21]]\n",
    "               \n",
    "        append_header_as_row(\n",
    "            file_name = 'población_mundial_2024.csv',\n",
    "            field_names = columnas\n",
    "            )\n",
    "        \n",
    "        # Extraer las filas de la tabla\n",
    "        table = response.xpath('//table//tbody//tr')\n",
    "        \n",
    "        for tb in table:\n",
    "            row_data = tb.xpath('.//td//text()').getall()            \n",
    "            row_data = [data.strip() for data in row_data] # limpia los espacios en blanco\n",
    "            \n",
    "            if len(row_data) == len(columnas):\n",
    "                dictionary = dict(zip(columnas, row_data))\n",
    "                append_dict_as_row(\n",
    "                    file_name = 'población_mundial_2024.csv', \n",
    "                    dict_of_elem = dictionary, # diccionario de valores\n",
    "                    field_names = columnas   # nombres de las columnas     \n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ejecución del spider**\n",
    "\n",
    "Volvemos a la raiz de la carpeta del proyecto:\n",
    "\n",
    "`cd..`\n",
    "\n",
    "Y ejecutamos el spider utilizando el nombre con que lo hayamos definido. En nuestro caso \"name = 'pob_mundial'\":\n",
    "\n",
    "`scrapy crawl pob_mundial`\n",
    "\n",
    "El spider inspeccionará la página web en busca de los elementos que le he pedido a través de los XPath y con la función **parse** extraerá los datos y los guardará en un archivo csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cargar en un DataFrame**\n",
    "\n",
    "Cargamos el archivo en un DataFrame para visualizar mejor los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#</th>\n",
       "      <th>Country (or dependency)</th>\n",
       "      <th>Population (2024)</th>\n",
       "      <th>Yearly Change</th>\n",
       "      <th>Net Change</th>\n",
       "      <th>Density (P/Km²)</th>\n",
       "      <th>Land Area (Km²)</th>\n",
       "      <th>Migrants (net)</th>\n",
       "      <th>Fert. Rate</th>\n",
       "      <th>Med. Age</th>\n",
       "      <th>Urban Pop %</th>\n",
       "      <th>World Share</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>India</td>\n",
       "      <td>1,450,935,791</td>\n",
       "      <td>0.89 %</td>\n",
       "      <td>12,866,195</td>\n",
       "      <td>488</td>\n",
       "      <td>2,973,190</td>\n",
       "      <td>-630,830</td>\n",
       "      <td>2.0</td>\n",
       "      <td>28</td>\n",
       "      <td>37 %</td>\n",
       "      <td>17.78 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>China</td>\n",
       "      <td>1,419,321,278</td>\n",
       "      <td>-0.23 %</td>\n",
       "      <td>-3,263,655</td>\n",
       "      <td>151</td>\n",
       "      <td>9,388,211</td>\n",
       "      <td>-318,992</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40</td>\n",
       "      <td>66 %</td>\n",
       "      <td>17.39 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>United States</td>\n",
       "      <td>345,426,571</td>\n",
       "      <td>0.57 %</td>\n",
       "      <td>1,949,236</td>\n",
       "      <td>38</td>\n",
       "      <td>9,147,420</td>\n",
       "      <td>1,286,132</td>\n",
       "      <td>1.6</td>\n",
       "      <td>38</td>\n",
       "      <td>82 %</td>\n",
       "      <td>4.23 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Indonesia</td>\n",
       "      <td>283,487,931</td>\n",
       "      <td>0.82 %</td>\n",
       "      <td>2,297,864</td>\n",
       "      <td>156</td>\n",
       "      <td>1,811,570</td>\n",
       "      <td>-38,469</td>\n",
       "      <td>2.1</td>\n",
       "      <td>30</td>\n",
       "      <td>59 %</td>\n",
       "      <td>3.47 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Pakistan</td>\n",
       "      <td>251,269,164</td>\n",
       "      <td>1.52 %</td>\n",
       "      <td>3,764,669</td>\n",
       "      <td>326</td>\n",
       "      <td>770,880</td>\n",
       "      <td>-1,401,173</td>\n",
       "      <td>3.5</td>\n",
       "      <td>20</td>\n",
       "      <td>34 %</td>\n",
       "      <td>3.08 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>230</td>\n",
       "      <td>Montserrat</td>\n",
       "      <td>4,389</td>\n",
       "      <td>-0.70 %</td>\n",
       "      <td>-31</td>\n",
       "      <td>44</td>\n",
       "      <td>100</td>\n",
       "      <td>-7</td>\n",
       "      <td>1.4</td>\n",
       "      <td>42</td>\n",
       "      <td>11 %</td>\n",
       "      <td>0.00 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>231</td>\n",
       "      <td>Falkland Islands</td>\n",
       "      <td>3,470</td>\n",
       "      <td>-0.20 %</td>\n",
       "      <td>-7</td>\n",
       "      <td>0</td>\n",
       "      <td>12,170</td>\n",
       "      <td>-13</td>\n",
       "      <td>1.7</td>\n",
       "      <td>42</td>\n",
       "      <td>68 %</td>\n",
       "      <td>0.00 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>232</td>\n",
       "      <td>Tokelau</td>\n",
       "      <td>2,506</td>\n",
       "      <td>4.55 %</td>\n",
       "      <td>109</td>\n",
       "      <td>251</td>\n",
       "      <td>10</td>\n",
       "      <td>72</td>\n",
       "      <td>2.6</td>\n",
       "      <td>27</td>\n",
       "      <td>0 %</td>\n",
       "      <td>0.00 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>233</td>\n",
       "      <td>Niue</td>\n",
       "      <td>1,819</td>\n",
       "      <td>0.11 %</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>260</td>\n",
       "      <td>10</td>\n",
       "      <td>2.5</td>\n",
       "      <td>36</td>\n",
       "      <td>44 %</td>\n",
       "      <td>0.00 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>234</td>\n",
       "      <td>Holy See</td>\n",
       "      <td>496</td>\n",
       "      <td>0.00 %</td>\n",
       "      <td>0</td>\n",
       "      <td>1,240</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>1.0</td>\n",
       "      <td>59</td>\n",
       "      <td>N.A.</td>\n",
       "      <td>0.00 %</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>234 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       # Country (or dependency) Population (2024) Yearly Change  Net Change  \\\n",
       "0      1                   India     1,450,935,791        0.89 %  12,866,195   \n",
       "1      2                   China     1,419,321,278       -0.23 %  -3,263,655   \n",
       "2      3           United States       345,426,571        0.57 %   1,949,236   \n",
       "3      4               Indonesia       283,487,931        0.82 %   2,297,864   \n",
       "4      5                Pakistan       251,269,164        1.52 %   3,764,669   \n",
       "..   ...                     ...               ...           ...         ...   \n",
       "229  230              Montserrat             4,389       -0.70 %         -31   \n",
       "230  231        Falkland Islands             3,470       -0.20 %          -7   \n",
       "231  232                 Tokelau             2,506        4.55 %         109   \n",
       "232  233                    Niue             1,819        0.11 %           2   \n",
       "233  234                Holy See               496        0.00 %           0   \n",
       "\n",
       "    Density (P/Km²) Land Area (Km²) Migrants (net)  Fert. Rate  Med. Age  \\\n",
       "0               488       2,973,190       -630,830         2.0        28   \n",
       "1               151       9,388,211       -318,992         1.0        40   \n",
       "2                38       9,147,420      1,286,132         1.6        38   \n",
       "3               156       1,811,570        -38,469         2.1        30   \n",
       "4               326         770,880     -1,401,173         3.5        20   \n",
       "..              ...             ...            ...         ...       ...   \n",
       "229              44             100             -7         1.4        42   \n",
       "230               0          12,170            -13         1.7        42   \n",
       "231             251              10             72         2.6        27   \n",
       "232               7             260             10         2.5        36   \n",
       "233           1,240               0             18         1.0        59   \n",
       "\n",
       "    Urban Pop % World Share  \n",
       "0          37 %     17.78 %  \n",
       "1          66 %     17.39 %  \n",
       "2          82 %      4.23 %  \n",
       "3          59 %      3.47 %  \n",
       "4          34 %      3.08 %  \n",
       "..          ...         ...  \n",
       "229        11 %      0.00 %  \n",
       "230        68 %      0.00 %  \n",
       "231         0 %      0.00 %  \n",
       "232        44 %      0.00 %  \n",
       "233        N.A.      0.00 %  \n",
       "\n",
       "[234 rows x 12 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('./poblacion_mundial/población_mundial_2024.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Conclusiones**\n",
    "\n",
    "El uso de Scrapy para realizar web scraping en una web que presenta datos de población mundial en una tabla estática ha demostrado ser una solución eficaz y eficiente. A través de este proceso, se han identificado varias ventajas clave del uso de Scrapy para este tipo de tareas, así como consideraciones importantes para futuros proyectos.\n",
    "\n",
    "**Eficiencia y Velocidad**\n",
    "\n",
    "- *Rendimiento Óptimo*: Scrapy es altamente eficiente para la extracción de datos de tablas estáticas. Su capacidad para gestionar múltiples solicitudes en paralelo permite la recolección rápida de grandes volúmenes de datos sin sobrecargar los recursos del sistema ni los servidores objetivo.\n",
    "\n",
    "- *Simplicidad en la Implementación*: Trabajar con contenido estático facilita enormemente el desarrollo de spiders en Scrapy. La simplicidad del HTML estático reduce la complejidad del código, permitiendo una implementación rápida y directa sin la necesidad de manejar JavaScript o formularios dinámicos.\n",
    "\n",
    "**Precisión en la Extracción**\n",
    "\n",
    "- *Selección de Datos*: Scrapy permite una extracción precisa de los datos mediante el uso de selectores CSS y XPath. Esto asegura que solo se recojan los datos relevantes de la tabla, minimizando la necesidad de limpieza de datos posterior.\n",
    "- *Automatización y Escalabilidad*: Scrapy automatiza de manera efectiva el proceso de scraping, lo que es especialmente útil para proyectos que requieren actualizaciones periódicas de datos. Su arquitectura escalable permite ajustarse fácilmente a diferentes volúmenes de datos y estructuras de sitios web.\n",
    "\n",
    "**Manejo de Estructuras Complejas**\n",
    "\n",
    "- *Navegación por HTML*: Aunque en este caso se trabajó con una tabla estática, Scrapy es capaz de manejar estructuras HTML complejas con facilidad. Esto lo hace adecuado no solo para tablas simples, sino también para proyectos donde los datos están dispersos en diferentes partes de una página o en múltiples páginas vinculadas.\n",
    "- *Facilidad de Expansión*: Scrapy facilita la adición de nuevas funcionalidades, como el manejo de varias páginas, la inclusión de datos de múltiples fuentes, o el procesamiento y almacenamiento de datos en diferentes formatos (como JSON, CSV, o bases de datos).\n",
    "\n",
    "**Consideraciones Éticas y Legales**\n",
    "\n",
    "- *Cumplimiento de Normativas*: Es fundamental realizar el scraping de manera ética, respetando las políticas del sitio web, como las directrices del archivo \"robots.txt\". Además, es importante asegurarse de que los datos obtenidos sean utilizados de manera responsable y conforme a las regulaciones vigentes en cuanto a la recolección y uso de datos.\n",
    "\n",
    "**Robustez y Mantenimiento**\n",
    "\n",
    "- *Mantenimiento y Actualización*: Scrapy permite crear spiders robustos que pueden mantenerse y actualizarse fácilmente. Si la estructura de la página web cambia, Scrapy facilita la adaptación del código para continuar extrayendo los datos de manera eficiente.\n",
    "\n",
    "- *Documentación y Comunidad*: Scrapy cuenta con una excelente documentación y una comunidad activa, lo que es un gran soporte para resolver dudas y mejorar la implementación. Esto asegura que incluso los desarrolladores menos experimentados puedan avanzar en sus proyectos de scraping con confianza.\n",
    "\n",
    "En resumen, Scrapy es una herramienta poderosa y confiable para la extracción de datos de tablas estáticas en la web. Su eficiencia, facilidad de uso, y capacidad para manejar estructuras de datos complejas lo hacen ideal para proyectos de scraping que requieren precisión y escalabilidad. Además, su enfoque modular y su sólida comunidad de soporte aseguran que Scrapy continuará siendo una herramienta clave para la extracción de datos web en futuros proyectos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Referencias**\n",
    "\n",
    "- [Python](https://www.python.org/)\n",
    "- [Ipykernel](https://pypi.org/project/ipykernel/)\n",
    "- [tqdm](https://pypi.org/project/tqdm/)\n",
    "- [joblib](https://pypi.org/project/joblib/)\n",
    "- [scipy](https://scipy.org/)\n",
    "- [Pandas](https://pandas.pydata.org/)\n",
    "- [Documentación oficial de Scrapy](https://docs.scrapy.org/en/latest/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "publiscrapy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
